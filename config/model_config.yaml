# Model Configuration File
# ===========================================================
# This file contains the configuration for model parameters and settings  

dataset_name: 'credit_card_fraud'
# Spiking Neural Network (SNN) parameters
snn_model:
  enabled: True 
  model_name: 'SNN Early Warning System '
  layer_types: ['Linear', 'LIF']
  input_size: 29 # Number of input features
  time_steps: 50 
  hidden_layers: 
    #- units: 128
    #  activation: 'LIF'
    #  beta: 0.97 #Leakage parameter for LIF neurons
    #  tou : 2.0 # Time constant for LIF neurons
    - units: 32
      activation: 'LIF'
      beta: 0.95 #Leakage parameter for LIF neurons
      tou : 2.0 # Time constant for LIF neurons
  output_size: 32
  threshold: 0.87 # Threshold for firing neurons
  sparse_gradient: True # Use sparse gradients for efficiency

# Conventional Neural Network (CNN) parameters
conventional_nn_model:
  mode_name: 'Compressed Fraud Detector CNN'
  enabled: True
  input_size: 32 # Input from SNN output_size
  architecture_type: 'MLP'
  mlp_layers:
    - units: 128
      activation: "ReLU"
      dropout_rate: 0.2
      batchnorm: False
    # - out_channels: 128
    #   kernel_size: 3
    #   padding: 1
  output_size: 1 # Binary classification for fraud detection
  output_activation: 'Sigmoid' # For binary classification
xgb_cnn_bilstm_model:
  mode_name: 'XGBoost + CNN + BiLSTM Fraud Detector'
  enabled: False
  input_size: 32                  # input features
  cnn_config:
    cnn_layers:
      - out_channels: 32
        kernel_size: 3
        padding: 1
        pool_size: 2
        dropout_rate: 0.1
      - out_channels: 64
        kernel_size: 3
        padding: 1
        pool_size: 2
        dropout_rate: 0.1
  lstm_config:
    hidden_size: 64
    num_layers: 1
    bidirectional: True
    dropout: 0.2
  mlp_config:
    mlp_layers:
      - units: 256
        activation: 'ReLU'
        dropout_rate: 0.3
        batchnorm: True
      - units: 128
        activation: 'ReLU'
        dropout_rate: 0.2
        batchnorm: True
  threshold_tuning:
    enabled: True
    search_range: [0.80, 0.95, 0.01]  # search thresholds from 0.80 to 0.95

