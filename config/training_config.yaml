# Training Configuration
# --------------------

training_params:
  epochs: 50
  batch_size: 256
  learning_rate: 0.001
  optimizer: "Adam" # Options: "Adam", "SGD", "Adagrad"
  loss_function: "BCEWithLogitsLoss" # For binary classification
  # If imbalance is handled via class weights in loss function:
  use_class_weights: True
  class_weights_scale: 10.0 # Factor to scale minority class weight
  early_stopping:
    enabled: True
    patience: 10 # Number of epochs with no improvement before stopping
    min_delta: 0.0001 # Minimum change to qualify as an improvement
    monitor: "val_f1_score" # Metric to monitor for early stopping
  gradient_clipping:
    enabled: True
    clip_value: 1.0 # Clip gradients to prevent exploding gradients
  device: "mps" # Options: "cpu", "cuda", "mps" (for Apple Silicon Macs)
  num_workers: 4 # For data loading
  seed: 42 # Global random seed for reproducibility
  log_interval: 100 # Log progress every N batches/steps
  model_save_path: "models/checkpoints/"
  best_model_filename: "best_hybrid_fraud_detector.pth"
