# Training Configuration
# --------------------
training_params:
  epochs: 1
  batch_size: 256
  learning_rate: 0.0001
  optimizer: "Adam" # Options: "Adam", "SGD", "Adagrad"
  loss_config:
    type: 'BCEWithLogitsLoss' # BCEWithLogitsLoss - FocalLoss
    alpha: 0.25
    gamma: 2.0
  use_class_weights: False
  monitor_metric: 'f2_score'
  class_weights_scale: 10.0 # Factor to scale minority class weight
  early_stopping:
    enabled: True
    patience: 7 # Number of epochs with no improvement before stopping
    min_delta: 0.0001 # Minimum change to qualify as an improvement
  gradient_clipping:
    enabled: True
    clip_value: 1.0 # Clip gradients to prevent exploding gradients
  device: "cpu" # Options: "cpu", "cuda", "mps" (for Apple Silicon Macs)
  num_workers: 4 # For data loading
  log_interval: 100 # Log progress every N batches/steps
  model_save_path: "models/checkpoints/"
  best_model_hybrid_filename: "best_hybrid_fraud_detector.pth"
  class_imbalance_handling: False # Options: "Wei_tedRandomSampler"


xgboost_params:
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.1
  subsample: 0.8
  colsample_bytree: 0.8
  gamma: 0.0

compression_params:
  pruning:
    tolerance: 0.004
    enabled: False
    method: "l1_unstructured"   
    amount: 0.25
    schedule: "gradual"         
    start_epoch: 3
    end_epoch: 6
    frequency: 1

  quantization:
    tolerance: 0.004
    enabled: False
    method: "qat"               
    start_epoch: 8
    observer_type: "moving_average_minmax" # "minmax" or "moving_average_minmax"
    quantization_scheme: "non-symmetric"

